{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ff60c8-f678-4e2b-ac59-ed24b66d81a6",
   "metadata": {},
   "source": [
    "# `texts` as Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7cb0a57-f403-431d-aebc-a97a2245ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "import random\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "340e7270-ce01-4674-98f7-00818611e821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Anyway... maybe I shouldnâ€™t be so serious with you. Maybe I should just ask you about your day and make you smile. I do tend to get a bit deep, though â€” that is my curse lol. Iâ€™m very playful as well, however ðŸ™ˆ\\n ',\n",
       " ' Idk-  I donâ€™t really try to meet people.  I just work, come home, sleep &amp;amp; then do it again the next day.  Maybe thatâ€™s some of the problem.  Who knows... Iâ€™m starting to just not gaf anymore.  Maybe a race war will kick off &amp;amp; Iâ€™ll have purpose then.  Fug it\\n ',\n",
       " ' I am nice.  Iâ€™m a good person.  I mean, Iâ€™m annoying at times Iâ€™m sure &amp;amp; I can get bitchy like any woman does-  but Iâ€™m not a cunt &amp;amp; Iâ€™m pretty chill.  Iâ€™ve never been with a non white.  Iâ€™ve only been with 2 people in my life... both were white.  I donâ€™t have herpes lmao!    I think Iâ€™m pretty average as far as looks go.  Iâ€™m not some Swedish super model, but Iâ€™m not fugly.  Iâ€™m in shape.  I donâ€™t have a nigger ass.   Idk-  Iâ€™m just a normal average lass kicking off my 30â€™s &amp;amp; who will apparently wage cuck until god takes me out of my misery\\n ',\n",
       " ' Also Iâ€™m a doomer lol.\\n ',\n",
       " ' Maybe you have the right idea by not bothering with it too much. I know itâ€™s caused me a hell of a lot of heartache of late. Maybe love will find us. Have a wonderful Christmas, Angel. Would give you a kiss under the mistletoe if I could ðŸ™ˆðŸ˜š\\n ']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA\n",
    "with open(\"texts.json\", 'r') as f:\n",
    "    texts = json.load(f)\n",
    "\n",
    "# Check to see that the data loaded as expected\n",
    "texts[30:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77cb640b-7bdf-45a1-9b55-4dde8c58e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15b44097-7176-460d-bc88-83baff732138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    parser = etree.HTMLParser()\n",
    "    tree = etree.fromstring(text, parser)\n",
    "    return etree.tostring(tree, encoding='unicode', method='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "afa1199c-97fc-429b-b808-b66292d041d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "threefive = remove_html_tags(texts[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03c0b04a-a0ce-46df-9c28-936ec39e9779",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Type 'NoneType' cannot be serialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plaintexts \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mremove_html_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plaintexts \u001b[38;5;241m=\u001b[39m [\u001b[43mremove_html_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "Cell \u001b[0;32mIn[50], line 4\u001b[0m, in \u001b[0;36mremove_html_tags\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m parser \u001b[38;5;241m=\u001b[39m etree\u001b[38;5;241m.\u001b[39mHTMLParser()\n\u001b[1;32m      3\u001b[0m tree \u001b[38;5;241m=\u001b[39m etree\u001b[38;5;241m.\u001b[39mfromstring(text, parser)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43metree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtostring\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43municode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32msrc/lxml/etree.pyx:3513\u001b[0m, in \u001b[0;36mlxml.etree.tostring\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Type 'NoneType' cannot be serialized."
     ]
    }
   ],
   "source": [
    "plaintexts = [remove_html_tags(i) for i in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea1dcea7-3583-45c5-8ec1-f34628360462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['are', 'you', 'working', 'today', ',', 'beautiful', 'girl', ',', 'or', 'are', 'you', 'hanging', 'with', 'fam', '?'], ['i', 'â€™', 'm', 'working', '.', 'the', 'only', 'days', 'i', 'have', 'off', 'for', 'the', 'next', '2', 'weeks', 'is', 'christmas', 'day', '&', 'amp', ';', 'amp', ';', 'new', 'year', 'â€™', 's', 'day', '.', ':', '(', '.', 'i', 'need', 'to', 'get', 'another', 'job', '.', 'this', 'one', 'is', 'killing', 'me']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all the texts\n",
    "words = [ nltk.tokenize.word_tokenize(text.lower()) for text in texts ]\n",
    "\n",
    "# And check to see that went according to plan\n",
    "print(words[20:22])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1af726-a70f-4bc7-b075-c7d20b693af3",
   "metadata": {},
   "source": [
    "With all our texts now rendered as a lists of tokens, let's take a look at how long the texts are. We're first just going to get a list of the token counts, then we will determine the texts that fall within number ranges that are generated by some initial hand inspection. My logic is, for example, that texts that are less than 20 words are not going to offer much in the way of narrative, and, as it turns out, the longer texts are stuffed with HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c530ed92-9dfc-4ecb-88c3-3c8097e8fa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47344\n",
      "13819\n",
      "6212\n",
      "1408\n",
      "394\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# Count the tokens in each text\n",
    "counts = [ len(i) for i in words ]\n",
    "\n",
    "# Collect the number of texts within a given length range\n",
    "print(sum([i < 20 for i in counts]))\n",
    "print(sum([20 < i < 50 for i in counts]))\n",
    "print(sum([50 < i < 100 for i in counts]))\n",
    "print(sum([100 < i < 200 for i in counts]))\n",
    "print(sum([200 < i < 500 for i in counts]))\n",
    "print(sum([500 < i for i in counts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "874f8f7f-6e88-48f5-bffc-675843aa79a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69193\n",
      "70596\n",
      "1403\n"
     ]
    }
   ],
   "source": [
    "# Let's add those totals up\n",
    "total_counts = 47344 + 13819 + 6212 + 1408 + 394 + 16\n",
    "print(total_counts)\n",
    "\n",
    "# And compare them against the total number of texts\n",
    "print(len(texts))\n",
    "\n",
    "# These two totals turn up a discrepancy (explained below)\n",
    "print( len(texts) - total_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f59dd2-e8be-4a5a-ba6b-462678f153f7",
   "metadata": {},
   "source": [
    "*Sigh*. 1403 texts are missing! For now, I am not going to worry about it. I have enough texts with which to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c236173d-dca4-4539-9360-6cd1d09cf72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_0 =  [ i for i in words if len(i) <  20 ]\n",
    "word_20 = [ i for i in words if 20 < len(i) <  50 ]\n",
    "word_50 = [ i for i in words if 50 < len(i) < 100 ]\n",
    "word100 = [ i for i in words if 100 < len(i) < 200 ]\n",
    "word200 = [ i for i in words if 200 < len(i) < 500 ]\n",
    "word500 = [ i for i in words if len(i) > 500 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6196de9-e6a0-4bca-b417-a75e20dc85bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21833\n"
     ]
    }
   ],
   "source": [
    "workable_lengths = len(word_20) + len(word_50) + len(word100) + len(word200)\n",
    "print(workable_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fafbe3e9-c58b-445d-9401-6ca22a2dd717",
   "metadata": {},
   "outputs": [],
   "source": [
    "workable = [ i for i in words if 20 < len(i) < 500 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1188f77a-a8f0-43fe-9c61-58bafd0799d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22326"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(workable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7bd22a5-496a-4995-be88-cc57990b5071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(workable[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "78dd91a3-cd53-4020-bbe1-3e35a34eb31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you 're piquing my interest . i want to know what podcast you 're talking about . you do n't have to tell me , but i wan na know .\n",
      "matk schlieder @ matkschlieder Â· 8s replying to @ leoncosewgirl < a href= '' https : //gab.com/laylaalisha11 '' class= '' u-url mention '' rel= '' nofollow noopener noreferrer '' target= '' _blank '' > @ laylaalisha11 < /a > and 2 others but the lockdowns are not and never have been about the virus , they are about control and obeying the deep state psyop and the demands of the chicoms so as to take over the country and our economy . and it â€™ s still continuing due to the hysteria and cowardice of the people .\n",
      "hey dan . you do great work . i am migrating off twitter . just followed you on parler as well .\n"
     ]
    }
   ],
   "source": [
    "for i in random.sample(workable, 3):\n",
    "    print(\" \".join(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c063209f-ce44-40a5-90c1-1a0a9060e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"workable.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(workable, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5df0408b-250e-4b7a-95fd-e96ac586053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"workable.json\", 'w') as f:\n",
    "    json.dump(workable, f, indent=2) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
